{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# from environment import Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviroment:\n",
    "    def __init__(self, expected_request_lambda_1: float, expected_request_lambda_2: float, expected_return_lambda_1: float, expected_return_lambda_2: float, agent: Agent):\n",
    "        # initialize Poisson random variables\n",
    "        self.expected_request_lambda_1 = expected_request_lambda_1\n",
    "        self.expected_request_lambda_2 = expected_request_lambda_2\n",
    "\n",
    "        self.expected_return_lambda_1 = expected_return_lambda_1\n",
    "        self.expected_return_lambda_2 = expected_return_lambda_2\n",
    "\n",
    "        # intiailize agent\n",
    "        self.agent = agent\n",
    "\n",
    "    def get_rental_requests(self) -> tuple:\n",
    "        \"\"\"\n",
    "        -> returns a tuple of the number of rental requests at each location\n",
    "        \"\"\"\n",
    "        rental_request_1 = np.random.poisson(\n",
    "            lam=self.expected_request_lambda_1)\n",
    "        rental_request_2 = np.random.poisson(\n",
    "            lam=self.expected_request_lambda_2)\n",
    "\n",
    "        return (rental_request_1, rental_request_2)\n",
    "\n",
    "    def get_customer_returns(self) -> tuple:\n",
    "        \"\"\"\n",
    "        -> returns a tuple of the number of cars returning at each location\n",
    "        \"\"\"\n",
    "        customer_return_1 = np.random.poisson(\n",
    "            lam=self.expected_return_lambda_1)\n",
    "        customer_return_2 = np.random.poisson(\n",
    "            lam=self.expected_return_lambda_2)\n",
    "\n",
    "        return (customer_return_1, customer_return_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_request_lambda_1 = 3\n",
    "expected_request_lambda_2 = 4\n",
    "\n",
    "expected_return_lambda_1 = 3 \n",
    "expected_return_lambda_2 = 2\n",
    "\n",
    "# -> at location 1, all cars rented are returned\n",
    "# -> at location 2, only half the cars rented are expected to be returned\n",
    "\n",
    "environment = Enviroment(expected_request_lambda_1=expected_request_lambda_1, expected_request_lambda_2=expected_request_lambda_2, expected_return_lambda_1=expected_return_lambda_1, expected_return_lambda_2=expected_return_lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## run this cell to see samples of how the environment generate rental requests and customer returns \n",
    "\n",
    "# for i in range(days): \n",
    "#     request1, request2 = environment.get_rental_requests()\n",
    "\n",
    "#     return1, return2 = environment.get_customer_returns()\n",
    "\n",
    "#     print(f\"Location 1 actually has {request1} rental requests and receives {return1} cars that the customers returns || {return1 - request1} cars gained\")\n",
    "#     print(f\"Location 2 actually has {request2} rental requests and receives {return2} cars that the customers returns || {return2 - request2} cars gained\")\n",
    "\n",
    "#     print(f\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: \n",
    "    def __init__(self, cars1, cars2, cars_max: int, actions: list, starting_policy: list, states: list, rewards: list, theta: float, gamma: float): \n",
    "        # numbers of cars\n",
    "        self.cars1 = cars1 \n",
    "        self.cars2 = cars2\n",
    "        self.cars_max = cars_max\n",
    "\n",
    "        # variables for reinforcement learing\n",
    "\n",
    "        # policy contains index into the action array \n",
    "        self.policy = np.array(starting_policy) if type(starting_policy) != np.ndarray else starting_policy\n",
    "\n",
    "        # contains the actual value of increasing / decreasing the cars of the 2 locations \n",
    "        self.actions = np.array(actions) if type(actions) != np.ndarray else actions\n",
    "\n",
    "        # each state represents the number of cars in both locations\n",
    "        self.states = np.array(states) if type(states) != np.ndarray else states \n",
    "\n",
    "        # type of rewards available\n",
    "        self.rewards = rewards\n",
    "\n",
    "        self.theta = theta \n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.state_policy_values = np.zeros_like(self.states) \n",
    "\n",
    "        # the available states are the starting number of cars of each location to moving all cars from 1 place to another \n",
    "\n",
    "    def get_reward(self, reward_index, number_of_cars: int): \n",
    "        return self.rewards[reward_index] * number_of_cars\n",
    "\n",
    "    def p(self, s, action_index: tuple, next_day_rental_request_1: int, next_day_rental_request_2: int) -> tuple:\n",
    "        # do not include return because returns are only effective till the next day and is therefore added to the number of cars at each location at the end of the loop\n",
    "\n",
    "        # the probability of getting to the next state is 1 -> return next state\n",
    "\n",
    "        # what if the next day request is the actual request, and we assume that we actually know the environment and how it works ? \n",
    "\n",
    "        # from the current state and selecting the action `action_index`, how does it go? \n",
    "        # cars1 = self.cars1 + self.actions[action_index]\n",
    "        # cars2 = self.cars2 - self.actions[action_index]\n",
    "\n",
    "        cars1 = s[0]\n",
    "        cars2 = s[1]\n",
    "\n",
    "        number_of_cars_moved = self.actions[action_index]\n",
    "\n",
    "        if cars1 < np.abs(number_of_cars_moved) or cars2 < np.abs(number_of_cars_moved): \n",
    "            return None\n",
    "\n",
    "        cars1 +=  number_of_cars_moved\n",
    "        cars2 +=  number_of_cars_moved\n",
    "        \n",
    "        cost = self.get_reward(1, number_of_cars=np.abs(self.actions[action_index]))\n",
    "\n",
    "        final_reward = np.float32(0)\n",
    "\n",
    "        reward1 = self.get_reward(0, min(next_day_rental_request_1, cars1)) # if request > cars -> rent all cars. else, rent 'request' numbers of cars\n",
    "        reward2 = self.get_reward(0, min(next_day_rental_request_2, cars2)) # if request > cars -> rent all cars. else, rent 'request' numbers of cars\n",
    "\n",
    "        final_reward += reward1\n",
    "        final_reward += reward2\n",
    "        final_reward += cost\n",
    "\n",
    "        return final_reward, (cars1 - 1, cars2 - 1) \n",
    "\n",
    "    def get_action_from_policy(self, s: tuple) -> tuple: \n",
    "        \"\"\"\n",
    "        -> return the index into the action array \n",
    "        \"\"\"\n",
    "        return self.policy[s] \n",
    "\n",
    "    def set_action_to_policy(self, action_index: tuple, s: tuple) -> None: \n",
    "        \"\"\"\n",
    "        greedily set the new policy to the best action according to the value function \n",
    "        \"\"\"\n",
    "        self.policy[s] = action_index\n",
    "\n",
    "    def policy_evaluation_step(self, next_day_rental_request_1, next_day_rental_request_2) -> np.float32: \n",
    "        delta = np.float32(0) \n",
    "\n",
    "        for i in range(self.state_policy_values.shape[0]): \n",
    "            for j in range(self.state_policy_values.shape[1]): \n",
    "                s = (i, j)\n",
    "                v = self.state_policy_values[s]\n",
    "\n",
    "                action_index = self.get_action_from_policy(s)\n",
    "\n",
    "                # get reward and next state\n",
    "                reward, next_state = self.p(s, action_index=action_index, next_day_rental_request_1=next_day_rental_request_1, next_day_rental_request_2=next_day_rental_request_2)\n",
    "\n",
    "                # update current value function of current policy \n",
    "                self.state_policy_values[s] = reward + self.gamma * self.state_policy_values[next_state]\n",
    "\n",
    "                # calculate difference delta\n",
    "                delta = max(delta, np.abs(v - self.state_policy_values[s]))\n",
    "\n",
    "        return delta \n",
    "\n",
    "    def policy_evaluation(self, next_day_rental_request_1, next_day_rental_request_2) -> None: \n",
    "        while True: \n",
    "            delta = np.float32(0)\n",
    "\n",
    "            # loop through each state\n",
    "            # for i in range(self.state_policy_values.shape[0]): \n",
    "            #     for j in range(self.state_policy_values.shape[1]): \n",
    "                    # s = (i, j)\n",
    "                    # v = self.state_policy_values[s]\n",
    "\n",
    "                    # # get action index from policy \n",
    "                    # action_index = self.get_action_from_policy(s)\n",
    "\n",
    "                    # # get reward and next state\n",
    "                    # reward, next_state = self.p(s, action_index=action_index, next_day_rental_request_1=next_day_rental_request_1, next_day_rental_request_2=next_day_rental_request_2)\n",
    "\n",
    "                    # # update current value function of current policy \n",
    "                    # self.state_policy_values[s] = reward + self.gamma * self.state_policy_values[next_state]\n",
    "\n",
    "            delta = max(delta, self.policy_evaluation_step(next_day_rental_request_1, next_day_rental_request_2))\n",
    "\n",
    "            if delta < self.theta: \n",
    "                break \n",
    "\n",
    "        return \n",
    "\n",
    "    def policy_improvement_step(self, next_day_rental_request_1, next_day_rental_request_2) -> bool: \n",
    "        \"\"\"\n",
    "        -> returns True if the best policy is found, returns false if the policy is not found\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.state_policy_values.shape[0]): \n",
    "            for j in range(self.state_policy_values.shape[1]): \n",
    "                s = (i, j)\n",
    "                a = self.get_action_from_policy(s)\n",
    "\n",
    "                # getting the best action for the current state based on the value function. argmax_a p(r, s' | s, a) \n",
    "                best_action_index = np.zeros(2) \n",
    "                best_action_value = np.float32(0)\n",
    "\n",
    "                for x in range(self.actions.shape[0]): \n",
    "                    for y in range(self.actions.shape[1]): \n",
    "                        action_index = (x, y) \n",
    "                        reward, next_state = self.p(s=s, action_index=action_index, next_day_rental_request_1=next_day_rental_request_1, next_day_rental_request_2=next_day_rental_request_2)\n",
    "\n",
    "                        current_action_value = reward + self.state_policy_values[next_state]\n",
    "\n",
    "                        if current_action_value > best_action_value: \n",
    "                            best_action_value = current_action_value\n",
    "                            best_action_index = action_index\n",
    "\n",
    "                self.set_action_to_policy(best_action_index, s)\n",
    "\n",
    "                if a != best_action_index: \n",
    "                    return False\n",
    "                else: \n",
    "                    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(cars1=1, cars2=1, cars_max=3, actions=np.arange(0, 4, 1), starting_policy=np.zeros((3, 3)), states=np.zeros((3, 3)), rewards=[0, 10, -2], theta=1e-3, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Environment & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
