{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "# from environment import Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deivation for Poisson distribution with any arbitrary $\\lambda$ is $\\sqrt{\\lambda}$. \n",
    "\n",
    "In calculating the theoretical average of the distibution for policy evaluation, a practical range of number one should consider are values up to **3 to 4 standard deivations above the mean**. \n",
    "\n",
    "Therefore, the range one should consider can be expressed as $[0, \\lambda + 3 \\times \\sqrt{\\lambda}]$. Of course, the distribution can have values of larger than this range, but their probabilities are so small that it is inconsequential to the theoretical average of the policy evaluation step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: \n",
    "    def __init__(self, cars_max: int, actions: list, starting_policy: list, states: list, rewards: list, theta: float, gamma: float): \n",
    "        # numbers of cars\n",
    "        self.cars_max = cars_max\n",
    "\n",
    "        # variables for reinforcement learing\n",
    "\n",
    "        # policy contains index into the action array \n",
    "        self.policy = np.array(starting_policy) if type(starting_policy) != np.ndarray else starting_policy\n",
    "\n",
    "        # contains the actual value of increasing / decreasing the cars of the 2 locations \n",
    "        self.actions = np.array(actions) if type(actions) != np.ndarray else actions\n",
    "\n",
    "        # each state represents the number of cars in both locations\n",
    "        self.states = np.array(states) if type(states) != np.ndarray else states \n",
    "\n",
    "        # type of rewards available\n",
    "        self.rewards = rewards\n",
    "\n",
    "        self.theta = theta \n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.state_policy_values = np.zeros_like(self.states) \n",
    "\n",
    "        # the available states are the starting number of cars of each location to moving all cars from 1 place to another \n",
    "\n",
    "    def get_reward(self, reward_index, number_of_cars: int): \n",
    "        return self.rewards[reward_index] * number_of_cars\n",
    "\n",
    "    def p(self, s, action_index: tuple, today_rental_request_1: int, today_rental_request_2: int, today_customer_return_1, today_customer_return_2) -> tuple:\n",
    "        # do not include return because returns are only effective till the next day and is therefore added to the number of cars at each location at the end of the loop\n",
    "\n",
    "        # the probability of getting to the next state is 1 -> return next state\n",
    "\n",
    "        # what if the next day request is the actual request, and we assume that we actually know the environment and how it works ? \n",
    "\n",
    "        # from the current state and selecting the action `action_index`, how does it go? \n",
    "        # cars1 = self.cars1 + self.actions[action_index]\n",
    "        # cars2 = self.cars2 - self.actions[action_index]\n",
    "\n",
    "        cars1 = s[0] \n",
    "        cars2 = s[1] \n",
    "\n",
    "        # calculate rewards based on today's number of cars\n",
    "        number_of_cars_moved = self.actions[action_index]\n",
    "\n",
    "        if cars1 < np.abs(number_of_cars_moved) or cars2 < np.abs(number_of_cars_moved): \n",
    "            return None\n",
    "\n",
    "        cars1 +=  number_of_cars_moved\n",
    "        cars2 +=  number_of_cars_moved\n",
    "        \n",
    "        cost = self.get_reward(1, number_of_cars=np.abs(self.actions[action_index]))\n",
    "\n",
    "        final_reward = np.float32(0)\n",
    "\n",
    "        reward1 = self.get_reward(0, min(today_rental_request_1, cars1)) # if request > cars -> rent all cars. else, rent 'request' numbers of cars\n",
    "        reward2 = self.get_reward(0, min(today_rental_request_2, cars2)) # if request > cars -> rent all cars. else, rent 'request' numbers of cars\n",
    "\n",
    "        final_reward += reward1\n",
    "        final_reward += reward2\n",
    "        final_reward += cost\n",
    "\n",
    "        # calculate next state using the number of cars returned\n",
    "        cars1 += today_customer_return_1\n",
    "        cars2 += today_customer_return_2\n",
    "\n",
    "        cars1 = min(cars1, self.cars_max)\n",
    "        cars2 = min(cars2, self.cars_max)\n",
    "\n",
    "        return final_reward, (cars1 - 1, cars2 - 1) \n",
    "\n",
    "    def get_action_from_policy(self, s: tuple) -> tuple: \n",
    "        \"\"\"\n",
    "        -> return the index into the action array \n",
    "        \"\"\"\n",
    "        return self.policy[s] \n",
    "\n",
    "    def set_action_to_policy(self, action_index: tuple, s: tuple) -> None: \n",
    "        \"\"\"\n",
    "        greedily set the new policy to the best action according to the value function \n",
    "        \"\"\"\n",
    "        self.policy[s] = action_index\n",
    "\n",
    "    def check_valid_value_function_delta(self, delta) -> bool: \n",
    "        if delta < self.theta: \n",
    "            return True \n",
    "        else: \n",
    "            return False\n",
    "\n",
    "    def policy_evaluation_step(self, today_rental_request_1, today_rental_request_2, today_customer_return_1, today_customer_return_2) -> np.float32: \n",
    "        delta = np.float32(0) \n",
    "\n",
    "        for i in range(self.state_policy_values.shape[0]): \n",
    "            for j in range(self.state_policy_values.shape[1]): \n",
    "                s = (i, j)\n",
    "                v = self.state_policy_values[s]\n",
    "\n",
    "                action_index = self.get_action_from_policy(s)\n",
    "\n",
    "                # get reward and next state\n",
    "                reward, next_state = self.p(s, action_index=action_index, today_rental_request_1=today_rental_request_1, today_rental_request_2=today_rental_request_2, today_customer_return_1=today_customer_return_1, today_customer_return_2=today_customer_return_2)\n",
    "\n",
    "                # update current value function of current policy \n",
    "                self.state_policy_values[s] = reward + self.gamma * self.state_policy_values[next_state]\n",
    "\n",
    "                # calculate difference delta\n",
    "                delta = max(delta, np.abs(v - self.state_policy_values[s]))\n",
    "\n",
    "        return delta \n",
    "\n",
    "    def policy_improvement_step(self, today_rental_request_1, today_rental_request_2, today_customer_return_1, today_customer_return_2) -> bool: \n",
    "        \"\"\"\n",
    "        -> returns True if the best policy is found, returns false if the policy is not found\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.state_policy_values.shape[0]): \n",
    "            for j in range(self.state_policy_values.shape[1]): \n",
    "                s = (i, j)\n",
    "                a = self.get_action_from_policy(s)\n",
    "\n",
    "                # getting the best action for the current state based on the value function. argmax_a p(r, s' | s, a) \n",
    "                best_action_index = np.zeros(2) \n",
    "                best_action_value = np.float32(0)\n",
    "\n",
    "                for x in range(self.actions.shape[0]): \n",
    "                    for y in range(self.actions.shape[1]): \n",
    "                        action_index = (x, y) \n",
    "                        reward, next_state = self.p(s=s, action_index=action_index, today_rental_request_1=today_rental_request_1, today_rental_request_2=today_rental_request_2, today_customer_return_1=today_customer_return_1, today_customer_return_2=today_customer_return_2)\n",
    "\n",
    "                        current_action_value = reward + self.state_policy_values[next_state]\n",
    "\n",
    "                        if current_action_value > best_action_value: \n",
    "                            best_action_value = current_action_value\n",
    "                            best_action_index = action_index\n",
    "\n",
    "                self.set_action_to_policy(best_action_index, s)\n",
    "\n",
    "                if a != best_action_index: \n",
    "                    return False\n",
    "                else: \n",
    "                    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviroment:\n",
    "    def __init__(self, expected_request_lambda_1: float, expected_request_lambda_2: float, expected_return_lambda_1: float, expected_return_lambda_2: float, agent: Agent):\n",
    "        # initialize Poisson random variables\n",
    "        self.expected_request_lambda_1 = expected_request_lambda_1\n",
    "        self.expected_request_lambda_2 = expected_request_lambda_2\n",
    "\n",
    "        self.expected_return_lambda_1 = expected_return_lambda_1\n",
    "        self.expected_return_lambda_2 = expected_return_lambda_2\n",
    "\n",
    "        # intiailize agent\n",
    "        self.agent = agent\n",
    "\n",
    "        # initialize policy history \n",
    "        self.policy_history = []\n",
    "\n",
    "    def get_rental_requests(self) -> tuple:\n",
    "        \"\"\"\n",
    "        -> returns a tuple of the number of rental requests at each location\n",
    "        \"\"\"\n",
    "        rental_request_1 = np.random.poisson(\n",
    "            lam=self.expected_request_lambda_1)\n",
    "        rental_request_2 = np.random.poisson(\n",
    "            lam=self.expected_request_lambda_2)\n",
    "\n",
    "        return (rental_request_1, rental_request_2)\n",
    "\n",
    "    def get_customer_returns(self) -> tuple:\n",
    "        \"\"\"\n",
    "        -> returns a tuple of the number of cars returning at each location\n",
    "        \"\"\"\n",
    "        customer_return_1 = np.random.poisson(\n",
    "            lam=self.expected_return_lambda_1)\n",
    "        customer_return_2 = np.random.poisson(\n",
    "            lam=self.expected_return_lambda_2)\n",
    "\n",
    "        return (customer_return_1, customer_return_2)\n",
    "\n",
    "    def train(self, number_of_days, max_iterations) -> int: \n",
    "        \"\"\"\n",
    "        -> returns the number of iteration needed to find an optimal policy, or the number of max iterations if multiple policies are found\n",
    "        \"\"\"\n",
    "        for iteration in tqdm(range(max_iterations), desc=\"Policy Check\", total=max_iterations): \n",
    "            # save old policy and initialize benchmark variables\n",
    "            benchmark_rental_request_1, benchmark_rental_request_2 = self.get_rental_requests()\n",
    "            benchmark_customer_return_1, benchmark_customer_return_2 = self.get_customer_returns() \n",
    "\n",
    "            self.policy_history.append(self.agent.policy)\n",
    "\n",
    "            # inner loop to evaluate value function\n",
    "            for d in tqdm(range(number_of_days), desc=\"Policy Evaluation\", total=number_of_days): \n",
    "                today_rental_request_1, today_rental_request_2 = self.get_rental_requests()\n",
    "                today_customer_return_1, today_customer_return_2 = self.get_customer_returns()\n",
    "\n",
    "                delta = self.agent.policy_evaluation_step(today_rental_request_1=today_rental_request_1, today_rental_request_2=today_rental_request_2, today_customer_return_1=today_customer_return_1, today_customer_return_2=today_customer_return_2)\n",
    "\n",
    "                if self.agent.check_valid_value_function_delta(delta=delta): \n",
    "                    break \n",
    "\n",
    "            # check if current policy is optimal policy\n",
    "            has_optimal_policy = self.agent.policy_improvement_step(today_rental_request_1=benchmark_rental_request_1, today_rental_request_2=benchmark_rental_request_2, today_customer_return_1=benchmark_customer_return_1, today_customer_return_2=benchmark_customer_return_2)\n",
    "\n",
    "            if has_optimal_policy: \n",
    "                return iteration\n",
    "\n",
    "        return max_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_request_lambda_1 = 3\n",
    "expected_request_lambda_2 = 4\n",
    "\n",
    "expected_return_lambda_1 = 3 \n",
    "expected_return_lambda_2 = 2\n",
    "\n",
    "# -> at location 1, all cars rented are returned\n",
    "# -> at location 2, only half the cars rented are expected to be returned\n",
    "\n",
    "environment = Enviroment(expected_request_lambda_1=expected_request_lambda_1, expected_request_lambda_2=expected_request_lambda_2, expected_return_lambda_1=expected_return_lambda_1, expected_return_lambda_2=expected_return_lambda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## run this cell to see samples of how the environment generate rental requests and customer returns \n",
    "\n",
    "# for i in range(days): \n",
    "#     request1, request2 = environment.get_rental_requests()\n",
    "\n",
    "#     return1, return2 = environment.get_customer_returns()\n",
    "\n",
    "#     print(f\"Location 1 actually has {request1} rental requests and receives {return1} cars that the customers returns || {return1 - request1} cars gained\")\n",
    "#     print(f\"Location 2 actually has {request2} rental requests and receives {return2} cars that the customers returns || {return2 - request2} cars gained\")\n",
    "\n",
    "#     print(f\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_cars = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_actions = np.stack([np.arange(0, max_number_of_cars + 1), np.arange(0, -max_number_of_cars - 1, step=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 0, -1, -2, -3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(cars_max=max_number_of_cars, actions=available_actions, starting_policy=np.zeros((3, 3)), states=np.zeros((3, 3)), rewards=[0, 10, -2], theta=1e-3, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Environment & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
